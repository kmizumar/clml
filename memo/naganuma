# -*- mode: org -*-
#+AUTHOR: NAGANUMA Shigeta
* テーマ
** グラフマイニング
*** 内部構造解析
構造をもったデータ（グラフ）の性質はその部分構造（部分グラフ）が担っている
という考えに基づき、グラフの特徴を定量化する。
部分構造のパターン数の爆発に対してどう対処するかが問題。
これに対して以下の二手法がある。
**** 頻出パターンマイニング
数え上げによる直接的なアプローチ、例えば部分パターンの出現数に対して閾値を設け、
Association Analysis におけるアプリオリアルゴリズムのような効率のよい数え上げアルゴリズムを用いる。
**** カーネル法
部分パターンの数え上げなどはしない間接的アプローチ。
SVM などに用いられるカーネル関数をグラフに定義することによって（グラフカーネル）
その特徴量、およびグラフ類似度を算出する。
*** 外部構造解析
ソーシャルネットワークやタンパク質の相互作用ネットワークなどの、
データ間の関係を表すネットワーク構造を解析する。
**** ノード分類
ノードのクラス（ラベル）について教師なし／半教師ありな方法で予測する。
- ラベル伝播法
いくつかのノードについてラベルが与えられたときに、ラベル未知のノードについて
ラベルを予測する。「隣り合うノードは同じクラスに属す」という仮定によってラベルづけをしていく。

- グラフクラスタリング
**** リンク予測
グラフのリンク構造が部分的に与えられたときに、残りの部分のリンク構造の予測を行う問題。
シンプルな方法の一つとしては、ペアワイズ分類のような二つのノードの判別問題として扱う手法、
すなわち、特徴ベクトルを各ノードに定義して、二ノード間にリンクがあるかどうかを
予測する手法である。
** 時系列解析
*** ガウス型状態空間モデルとして
- 標準的季節調整モデル + AR 成分
- 時変係数 AR モデル
 
*** 非ガウス型状態空間モデル
システムノイズ・観測ノイズをガウス型と仮定しない状態空間モデル。
システムの構造変化に伴うパラメータの急激な変化や異常値の処理に適している。
フィルタにベイズの定理を使っており、極めて広い応用範囲を持っている。
状態推定にガウス型ではカルマンフィルタを用いたが、
非ガウスではモンテカルロフィルタ(粒子フィルタ)を用いる。

** 各プラットフォームで ML の動作確認
Win32, Linux32/64, Solaris(tarski) など
  
* ネットワーク異常検出
NTTのネットワークは
- ノード 10 万
- 各ノードパラメータ 60 個( traffic 入出力情報など )
計 600 万項目 を 5 分毎 に採取。すなわち 288 X 600万 のサイズの行列が
毎日得られる。この膨大な情報を使ってなにかできないか？

- ネットワーク診断
0.5 変化に気づく
1   異変に気づく -> 未知の異変に気づけるような方法であることも重要
2   どこがおかしいかわかる

- リアルタイム性
最短 5 分で得られるデータに対してどの程度のリアルタイム性が必要か。

** 画像解析的手法 
画素 : ノード
画素の情報（ 輝度やRGBなど ） : ノードのパラメータ
と対応付けて考えれば、ある時点におけるネットワークは画像と捉えることができる。
そこに画像解析の技術( 一般物体認識、画像分類、ジェスチャー認識など )を用いれば、
ネットワークを俯瞰したスケールでの変化が捉えられるのではないだろうか。
*** 特徴量抽出
その際に重要なのが、各ノードのパラメータである。
60 個をそのまま用いるのも一つの手段ではあるが、それでは扱うデータ量が膨大になってしまう。
これらパラメータが同じ重要性をもつわけではなく、分類／認識に全く寄与しないものがあったり、
相互の関連を見るような値の方がより効率的である場合などがある。
そういった特徴量を抽出する機械学習手法としては、

 - PCA (主成分分析)
 - MDS (多次元尺度構成法)
 - ICA (独立成分分析)
 - ISOMAP (局所的な分布構造を考慮するように改良された多次元尺度構成法)
 - LE (Laplacian Eigenmap)

などが選択肢として考えられ、これらは一般に次元圧縮と呼ばれる。
さらに最近の手法としては NMF (非負行列因子分解) などもあり、
顔認識技術において高い精度を示している。

特徴量の条件としては

 - 時系列的な傾向も含意した量
 - 各ノード間で比較可能であること

が求められている。
一点目については、
各パラメータの t 単位時間前との差分や、自己相関関数値が最大のラグ、前期比、
などを 60 個のパラメータに加え、次元圧縮を行うなどの方法が考えられる。
また後述するパーティクルフィルターにおける状態量を特徴量として用いることも
考えられる。

二点目については、
第一に考えられるのは、全てのノードのデータをひとまとめにして次元圧縮を
適用すれば、一つの特徴空間上に全てのノードが表せるので普遍的な量となるが、
そもそもデータ量が膨大であるので、計算が現実的でないものとなってしまう可能性があるので、
なんらかのグループ化（ネットワークの階層で分けるなど）が必要である。
そうなった場合、グループ間でノードの特徴量に普遍性を持たせるためにはどうすればよいかは
調査する必要がある。
*** 画像解析
各ノードが適切な特徴量で表現できたとする。ネットワークに異常がない限り、
局所的には各ノードは似た特徴を示すと考えられる。たとえばあるノードで
トラフィック量が多ければ、その近辺のノードのトラフィック量も多いはずである。
そういった局所的な特徴の集合によって、各時点のネットワーク画像が表現できれば、
それらを分類／学習することで、ネットワーク画像に対する知見が得られ、

- ネットワークのトレンドの変化( "p2p が使われ出した" など)
- 異常な局所的特徴

などに気づくことができる。これは以下に示すような画像解析の手法で実現できると考えられる。
**** 局所特徴量
画像における物体検出の技術に局所特徴量というものがある。
局所特徴量とは画像の一部分の特徴を表すベクトルであり、
- 局所領域の切り出し
- 領域の特徴量算出
という手続きで算出され、一つの画像に対して複数個抽出される。
二つの画像で、同じ物体が違う位置にあっても、
同じ局所特徴量がそれぞれその物体に対して算出されるので、
同じ物体と認識することができる。

代表的な手法としては以下の二つが考えられる。
- SIFT (Scale Invariant Feature Transform)
近年盛んに研究されている局所特徴量で、スケール変化などに強い。
顔認識などに用いられている。

- HOG (Histogram of Oriented Gradients)
SIFT と類似した局所特徴量であるが、より大まかな物体形状を表現することが
可能であり、人の形や車の形を検出する手法として用いられる。

ネットワークにおける局所的な特徴は画像における物体ととらえれば、
この局所特徴量ベクトルで抽出できると考えられる。
その際、一つ注意すべき点は、ある画素に対して周囲の画素は上下左右に並列に配置されるが、
ノードにたいしてその周囲のノードというのは、そのネットワーク構造に従った配置となり、
階層関係も生じる。よってノードに対して近傍をどのように定義するかというのは工夫が必要である。

**** bag-of-visual-words
局所特徴量の集合によってネットワーク画像が表現できれば bag-of-visual-words という方法を用いて、
画像のカテゴリ分類が可能となる。
bag-of-visual-words とは自然言語処理における文書分類の手法 bag-of-words を
画像に適用したもので、各文書を語 (word) の集合で表すのと同様に、
各画像をその局所特徴量 (visual word) の集合で表し、文書分類の手法をそれに適用する。
- ただし、局所特徴量はベクトルで、語 word というのはカテゴリ型データである。
  局所特徴量をそのまま visual word とするのは難があり、
  ベクトル量子化という方法でカテゴリ型データに変換する必要がある。

今回の分類において困難な点は、各画像のカテゴリがあらかじめ与えられているわけではないことであるが、
近年研究されている文書分類の手法 hdp-lda によって解決できる。

- hdp-lda
hdp-lda は潜在意味解析の手法の一つである。
各文書とそれを表す語の集合をつなげる意味の存在を仮定し、
それら潜在する意味によって各文書を分類するものである。
画像におきかえて考えれば、各画像とそれを表す局所特徴量に潜在するカテゴリを見出すことができる。
さらにこの hdp-lda の強みはあらかじめカテゴリ数を経験的に指定しなくても、
適切なカテゴリ数も推定されることである。

**** 変化／異常検出
hdp-lda を用いることで、ネットワーク画像はカテゴリ分けされる。
各カテゴリがどういった意味をもつネットワークの状態なのかは、
実際にそれらを見て解析（ノードパラメータの値の分布を見るなど）をしてみる必要があるが、
典型的な状態であれば、一定数以上の要素数をもつカテゴリとなると考えられる。

新しく採取されたネットワークデータに対して、同様の処理を施し、
局所特徴量を算出すれば、
- どのカテゴリにその瞬間のネットワークが属すか、
- どのカテゴリにも属さない、すなわち今までにない局所特徴量のパターン
- そもそも今までにない局所特徴量
などのことがわかり、
ネットワークの変化／異常に気づくことができるのではないかと考えられる。

** 動画像解析的手法
前述の画像解析的手法では、ネットワークデータの時間的な連続性は考慮しておらず、
各時間単位ごとのネットワーク全体のパターンを記述、学習し、
これからくるデータに対して変化／異常の判別を行おうというものである。

以下では、ネットワークデータの時系列データとしての側面を考慮し、
時間的な連続性を仮定した手法を提案する。

*** 手法1:オプティカルフロー推定
**** 概要
オプティカルフローとは、"動物体" 解析の手法の一つであり、
時間的に連続する動画像データから求められた各点の速度ベクトルによる速度場のことである。
例 http://www.media.imit.chiba-u.jp/~kameda/b4study/typhoon.

例は台風の動画であり、各画素は輝度の情報である。オプティカルフロー推定によって、
輝度の時系列情報から物体（台風の雲）がどの方向にどれだけの程度で移動していくかを、
ベクトルを用いて可視化したものである。

**** 可視化
ネットワークノードの各パラメータ値は、近隣のノードに強く影響を及ぼし合い、
その値の大きさはある方向と速さをもって伝播すると考えられるので、
画素の輝度となぞらえて考えることができる。
オプティカルフロー推定を用いて各パラメータ値の増減が、
ネットワーク全体においてどう伝播していくかを可視化できれば、
人間のネットワーク全体に対する直観的理解／新たな発見に役立つのではないかと思われる。

課題としては、やはり特徴量抽出が挙げられる。
各パラメータごとにフロー図を作成していては時間単位毎に 60 図もできてしまうので、
良い特徴量を前述の PCA などの方法で求める必要がある。
しかし可視化する場合、ある程度人間に理解できる特徴量にしなければ意味はない。
単に全パラメータに対して特徴量抽出の機械的な手法を適用するだけでは、性質の異なるパラメータが
混在しているので、抽出した特徴量が何を表すのかが人間には理解できない可能性もある。
よってあらかじめパラメータを、
 - traffic 流量に関するパラメータ
 - ルーターの状態に関するパラメータ(CPU 使用率など)
などに分類し、それぞれに特徴量抽出を施すことで各分類を代表する値として特徴量を見ることが考えられる。

**** 物体追跡
オプティカルフローを用いれば、動画中の動く物体の追跡を行うこともできる。
たとえば監視カメラのビデオに動く人影が写り込むと、その人影の領域に同方向／同程度の
オプティカルフローベクトルが発生する。そのベクトルを用いて、人影領域を認識し、
その領域を追跡することができる。

ネットワークにおいては、あるノードの近傍は、同じようなパラメータ値(または特徴量)の推移をすると
推測される。それらノードのパラメータ値が伝播していく様は、まさに動く人影となぞらえて考える
ことができるのではないか。
よってオプティカルフローによる物体追跡の技術を適用して、過去のネットワークデータから、
後述するような方法で物体移動のパターンを記述し、分類することで、
パラメータ値(または特徴量)の伝播に対する知見が得られ、新たなデータに対して、

 - 既知のパターン
 - 稀なパターン
 - 未知のパターン

などの判断ができると考えられ、新たなネットワークの潮流発見や異常検出に役立つのではないか。

- 物体移動パターンの記述／分類
今回の物体移動のパターン分類は教師なしの機械学習と考えられるが、
その手法として一般的には クラスタリング がある。
その際まずどのようにそのパターンを記述するかが問題である。
経路、物体の規模、速度などをクラスタリングの手法にのるような形で記述し、
かつ類似のパターンがきちんとまとまるようなものにしなければならない。

クラスタリングの手法も
 - k-means
 - 階層型クラスタリング
 - Self-Organization-Map
 - OPTICS
 - Dirichlet Process Mixture (DPM)
など様々あり、適切なものを検討する必要がある。
この中ではノンパラメトリックな手法である DPM を使えば、
他の手法ではヒューリスティクスが必要なクラスタ数を自動的に適切な数に推定してくれる
（事前分布を選択する必要はあるが）ので、有力な方法であるかもしれない。
**** オプティカルフロー推定法
- 勾配法
急激なパラメータ変化に対して弱い。
- ブロックマッチング法
急激なパラメータ変化に対して強いが、全探索を行うので計算時間が膨大となる。

*** 手法2:パーティクルフィルター
**** 概要
ネットワークデータの時系列性を考慮したもう一つの手法として、パーティクルフィルターを提案する。
パーティクルフィルターとは、状態空間モデルという時系列モデリング法の一つであり、
非ガウス分布を粒子近似（モンテカルロ近似）により実現するので、そう呼ばれている。
トラッキングなどの画像処理、音声認識、遺伝子情報処理、時系列解析などといった
幅広い分野に応用されている。

状態空間モデルとは、観測データのモデルと、直接には観測されないが観測データ値を
決定づける状態モデル、の組によって表現されるモデルである。

- 例: 宇宙機の異常検出
パーティクルフィルタを用いた例として、宇宙機の異常検出がある。
ref http://jglobal.jst.go.jp/public/20090422/200902270638558229

この例では、
状態値として宇宙機の姿勢角と角速度など宇宙機外部から観測されるデータ
観測値として宇宙機に取り付けられた機器のパラメータ値
と定義している。
各機器は宇宙機の制動などの役割を担っており、観測値は状態値によって決定づけられる。

異常検出は、パーティクルフィルタによって算出した推定値と実際の観測値との誤差を評価する方法で行い、
 - 機器の故障による突発的な異常
 - 機器の劣化により徐々に顕れる異常
双方について迅速かつ正確に異常検出できることが示唆されている。

こういった現象をモデル化する上での問題として、対象物に対して起こりうる物理現象を全てモデル化できず、
不確定要因による値の影響により推定精度が低くなってしまうことがあるが、
この例では、不確定要因を表現する式を独自に状態空間モデルに加え、精度の高い推定を実現している。
**** 各ノードの状態
ネットワークにおける各ノードを宇宙機と捉えれば、パーティクルフィルタを用いて精度の高い
時系列モデル化を行い、各ノードの状態／観測値の推定を行うことが可能である。

ただここで問題になるのは、

- 状態値をどのように定義するか。
　宇宙機の場合は宇宙機自体の姿勢角や角速度といった宇宙機の外からの観測情報が得られたが、
  ノードにはそのようなものはない。
- モデルのパラメトリックな部分（状態モデルの分散値など）をどう設定するか

があり、特に一点目が困難な問題かと考えられる。
宇宙機の場合のように事前知識を用いてそれらしいものを自分で定義する方法以外にも、
機械学習手法で自動的に求める方法も提案されている。
ref. "Bayesian Inference for Linear Dynamic Models with Dirichlet Process Mixtures"
     F. Caron1 , M. Davy1 , A. Doucet2 , E. Duflos1 , P. Vanheeghe1

この方法を用いて、状態が推定でき各ノードのよい時系列モデルが得られれば、
宇宙機の例にあるように各ノードの異常検知も可能であると考えられる。
しかしそれ以上に今回の問題に寄与すると考えられるのは、ノードの状態モデルが得られることで、
それら状態値をノードの特徴量と捉えることができる点にある。
藤井さんによれば、この状態値を各ノード間で比較可能な値とすることもできる可能性が高いとの
ことであるので、各ノードの状態値を輝度と見なし、画像解析的手法の俎上に乗せることができるのではないか。

**** 物体追跡
また、オプティカルフローのときに述べた移動物体追跡の方法として、
パーティクルフィルタを用いる手法があり、オプティカルフローの場合と同様、
ネットワークにおけるなんらかの流動性のパターン分類ができ、変化／異常に気づくことができるのではないか。

しかしパーティクルフィルターを用いた移動物体追跡では、
背景画像との差分によって動物体を捉えんとしている。
背景画像とは動物体が何もない状態の全体の画像であるが、
ネットワークにおいてこの背景画像をどう定義するかは一つの問題である。

** グラフマイニング
ここまではネットワークを画像と見立て、画像におけるパターン認識手法を適用してきたが、
ネットワークを表現する自然なデータ構造の一つとして グラフ がある。
機械学習／データマイニングの手法を用いてグラフから知見を得ようとする研究は、
グラフマイニング、リンクマイニングなどと呼ばれ、盛んに行われている。

*** グラフ時系列を用いた異常検出 1
今回のようなネットワーク構造および各ノードパラメータの時系列情報が得られるような
データに対しての研究として、オンライン異常検知手法がある。
ref: Ide, Kashima "Eigenspace-based Anomaly Detecyion in Computer Systems"

この研究における異常検知とは、個々のノードの異常検知ではなく、
与えられたグラフに特徴ベクトルを定義し、その特徴ベクトルを見ることで
各時刻において正常か異常かの判断をするものである。
よってネットワークをある程度俯瞰した異常検知手法である。
**** Dependency matrix
構造自体は fix されているが、時間に応じてリンクの重みが変化するようなグラフがあったとする。
Dependency matrix とは、そのグラフの重み付き隣接行列のことである。
重みとは例えばノードAからノードBまでのパケット数などが入る。
**** Active vector
グラフに対応する Dependency matrix は、その時刻にどういう活動がそのグラフ内であったかを
表している。固有対を求めることで、その活動の特徴がベクトルとして求まる。
このベクトルを Active vector と呼ぶ。
**** Anomaly detection
異常検出は大まかに言えば、
Active vector の典型的な例を、いままでの時系列グラフから算出し、
現在のグラフから求められる特徴ベクトルとの類似度を評価することで、
異常かどうかを判断する。
***** typical activity pattern
典型的な特徴ベクトルは、各時刻の特徴ベクトルの線形結合によって得る。
線形結合における係数行列は SVD (特異値分解) によって求める。
***** anomaly metric
vMF(von Mises Fisher) 分布を用いることで、特徴ベクトルの典型的な
変動幅を算出することができる。
内積によって特徴ベクトルの類似度を定義し、先に求めた vMF 分布から
類似度の閾値を求めることができる。閾値以下となった場合は、
異常と判断する。
vMF 分布自体はオンライン学習することができ、閾値は動的な値となる。
**** 問題点
10 万ノードある NTT ネットワークデータからそのまま Dependency matrix を算出すると、
その構成方法からいって、少なくとも 2 倍以上のサイズになってしまうため、計算において
非現実的な規模と言わざるを得ない。
また上述の特徴ベクトルというのは一つのグラフに対して一つ求まるものであるが、
10 万ノード全体の特徴ベクトルが一つだけ求まる、というのはあまりに情報として大域的すぎる。

よって、もっと小さいスケールにグラフを切り分けてから、この手法を適用するのが望ましい。
機械学習的にグラフをお互いに疎な部分グラフに分割する手法としては
- スペクトラルクラスタリング や
- ref: 桑田、上田、山田 "ノンパラメトリックベイズモデルによるグラフクラスタリング
などが提案されている。
しかしそもそもが大規模であるので、機械学習的なグラフクラスタリング手法をそのまま用いる
のはおそらく困難であろう。よって
切り分けの第一段階として、知識（ここらへんのノードはある県に対応している、など）によって
大まかに分割し、第二段階でグラフクラスタリングの手法によって、
好ましいスケールのグラフに切り分け、各部分グラフに異常検出学習アルゴリズムを適用するのが
良いと考えられる。
*** グラフ時系列を用いた異常検出 2
NTT のネットワークシミュレーションにおいて、「グラフ時系列を用いた異常検出 1」の手法は、
対象グラフのどこかで発生した異常を検知することには威力を発揮した。
さらに異常箇所/リンクを特定しようと試みたが、「グラフ時系列を用いた異常検出 1」の範囲内では、
"複数個それらしいと思われる異常ノードを列挙しその中に異常リンクと関連するノードが入っている"
といった程度までしか至れなかった。

井出氏の関連研究で、車のセンサーデータのような、

- いくつかのパラメータが相互に強い相関を持っている
- システムが高度に動的 (試行によってデータがまったく違うトレンドを示す)

であるようなデータについて、異常パラメータを特定するという手法がある。
- T.Ide "Computing Correlation Anomaly Scores using Stochastic Nearest Neighbors"

パラメータ同士の相関関係をパラメータごとに グラフ で表現し、その崩れを検出することで、異常検出を行う。
通常時であれば各パラメータのグラフは不変である、という近傍保存原理と著者が呼ぶ前提に依っている。

NTT のネットワークシミュレーションにおいても、"各リンクのトラフィック量" をパラメータとすれば、
あるノードを介して隣り合うリンクや、階層関係があるリンクの間には強い相関があると考えられ、
本手法を適用すれば、異常リンク特定に至れるはずである。

**** 偽相関
実際に本手法を、NTTシミュレーションの東京１における 「リンク劣化故障」 について試してみたところ、
以下のような結果( 結果 1 )となった。結果は各パラメータ（ 各リンク ）の異常度合い( e-score ) が示される。
本手法では、グラフのサイズ k ( 関連のあるパラメータの数 ) を指定することが必要であり、結果 1 では k = 3 とした。

リンク劣化故障を起こしたのはノード 3 とノード 5 を結ぶ link 8 であるが、
関係のない link 10, link 12, link 14, ... , link 256 の e-score が高くなってしまっている。
そこで各パラメータのグラフ構造を調べて見たところ、次のような問題が見つかった。

- 本手法では解析の第一手として、各リンクのトラフィック量の 相互相関行列 を算出する。
  すると、本質的に特に関係のないところでも増減が似ていると相関があると判断されてしまう。
  NTTネットワークシミュレーションではノイズをまったく仮定しておらず、
  非常に "綺麗な" データなのでそういった箇所が多々ある。
  例えば 東京 1 だと、
  ノード 5, 9 のリンクにおけるトラフィック量と、
  ノード 6, 10 のリンクにおけるトラフィック量では
  その値の生成過程は独立であるが、平常時における値の増減傾向は全く同じであるため、
  高い相関値を示してしまう。このような偽相関がグラフのノイズとなってしまっている。

NTTネットワークシミュレーション では、
各 SSE に属す登録者数( subscriber )の割合に従って各 SSE を流れるトラフィック量を決めている。
よって平常時、各リンクを流れるトラフィック量の割合は一定であり、
本手法に想定される動的なシステムや現実のネットワークとは程遠い。
そこで、各SSEを流れるトラフィック量に ガウスノイズ を加え、
結果 1 のときと同様に東京 1 においてリンク劣化故障を試してみたところ
結果 2 のようになった。

結果 1 のときに問題となった link 10, link 12, ... , link 256 の e-score は著しく下がった。
ガウスノイズによる影響は SSE と 外部 をつなぐ経路にしか影響がないため、
トラフィック量に因果関係がないリンク同士の増減傾向が一致してしまうことが防がれたと考えられる。

一方で、故障対象リンクである link 8 の e-score も減少してしまった。
link 8 [3 5] の近傍構造を異常時といくつかの平常時で比較してみたところ、

- 異常時
Neighbor : Pji (coupling probability)
link 10 [3 6] : 0.32731772179276925
link 6 [1 3] : 0.2222683809508796
link 70 [5 73] : 0.0822398120353843

- 平常時 1
Neighbor : Pji (coupling probability)
link 10 [3 6] : 0.39869408679617435
link 70 [5 73] : 0.09719745782165332
link 136 [6 130] : 0.09408243521784625

- 平常時 2
Neighbor : Pji (coupling probability)
link 10 [3 6] : 0.4007885425285982
link 142 [6 124] : 0.11099453674154873
link 134 [5 9] : 0.07780997651745444

- 平常時 3
Neighbor : Pji (coupling probability)
link 10 [3 6] : 0.4039082407232531
link 142 [6 124] : 0.09241945816042824
link 136 [6 130] : 0.0893381507625011

まだまだノイズが多い。上に出てくるリンクの中で、
link 8 [3 5] と

- 実際に因果関係がある
  link 70 [5 73], link 134 [5 9]

- 偽相関である
  link 10 [3 6], link 136 [6 130], link 142 [6 124]

となっており、
特に link 10 [3 6] の Pji (coupling probability) がどの場合でも他と比較して高い値である。
link 8, link 10 は共に AER - BCR 間のリンクであり、ガウスノイズを付与した各 SSE からのリンクは
AER でまとめられており、ガウスノイズによる影響が均されてしまい、ノイズを加える以前と同様の
偽相関が link 8 と link 10 で起こってしまっていると考えられる。

相関関係を見る場合このような偽相関の問題は常につきまとう。
ヒュームという哲学者によれば、「因果関係は具体的推論に基づかない。観測できるのは相関関係のみ」
とまで言われており、観測データから因果関係を推定するのは困難な問題である。
リンクのトラフィック量以外の情報から因果関係がわかる（例えばノード間のトラフィックパスの有無など）
のであれば、その情報も解析に盛り込むのも一つの方法と考えられる。

現状、対策として以下が考えられる。

***** 非類似度行列を工夫
****** トラフィックパスの無いリンク間の非類似度を ∞ にする
例えば link 8 [3 5] と link 10 [3 6] を通るトラフィックパスは存在しないので、
非類似度行列でそれに対応する成分を十分に大きい数にする。

***** 偽相関を解消する統計的手法
****** 偏相関係数
パラメータ１、パラメータ２、パラメータ３のそれぞれの組で相関値が高いとする。
パラメータ３がパラメータ１とパラメータ２の共通原因となっている場合、
偏相関係数を用いて、パラメータ３の影響を除いたパラメータ１とパラメータ２の
相関値が求まる。
どのパラメータが共通原因になっているかは、因果推論に属す問題であり、
別の情報から判断しなければならない。

**** 結果 1
(("link 2 [1 2]" . 0.0) ("link 3 [3 4]" . 0.0) ("link 4 [5 7]" . 0.0) ("link 5 [6 8]" . 0.0)
 ("link 6 [1 3]" . 0.5000362256932298) ("link 7 [2 4]" . 0.0) ("link 8 [3 5]" . 0.5000000288469088)
 ("link 9 [4 7]" . 0.0) ("link 10 [3 6]" . 0.75) ("link 11 [4 8]" . 0.0) ("link 12 [5 131]" . 0.75)
 ("link 13 [7 131]" . 0.0) ("link 14 [5 129]" . 0.75) ("link 15 [7 129]" . 0.0) ("link 16 [5 127]" . 0.75)
 ("link 17 [7 127]" . 0.0) ("link 18 [5 125]" . 0.75) ("link 19 [7 125]" . 0.0) ("link 20 [5 123]" . 0.75)
 ("link 21 [7 123]" . 0.0) ("link 22 [5 121]" . 0.75) ("link 23 [7 121]" . 0.0) ("link 24 [5 119]" . 0.75)
 ("link 25 [7 119]" . 0.0) ("link 26 [5 117]" . 0.75) ("link 27 [7 117]" . 0.0) ("link 28 [5 115]" . 0.75)
 ("link 29 [7 115]" . 0.0) ("link 30 [5 113]" . 0.75) ("link 31 [7 113]" . 0.0) ("link 32 [5 111]" . 0.75)
 ("link 33 [7 111]" . 0.0) ("link 34 [5 109]" . 0.75) ("link 35 [7 109]" . 0.0) ("link 36 [5 107]" . 0.75)
 ("link 37 [7 107]" . 0.0) ("link 38 [5 105]" . 0.75) ("link 39 [7 105]" . 0.0) ("link 40 [5 103]" . 0.75)
 ("link 41 [7 103]" . 0.0) ("link 42 [5 101]" . 0.75) ("link 43 [7 101]" . 0.0) ("link 44 [5 99]" . 0.75)
 ("link 45 [7 99]" . 0.0) ("link 46 [5 97]" . 0.75) ("link 47 [7 97]" . 0.0) ("link 48 [5 95]" . 0.75)
 ("link 49 [7 95]" . 0.0) ("link 50 [5 93]" . 0.75) ("link 51 [7 93]" . 0.0) ("link 52 [5 91]" . 0.75)
 ("link 53 [7 91]" . 0.0) ("link 54 [5 89]" . 0.75) ("link 55 [7 89]" . 0.0) ("link 56 [5 87]" . 0.75)
 ("link 57 [7 87]" . 0.0) ("link 58 [5 85]" . 0.75) ("link 59 [7 85]" . 0.0) ("link 60 [5 83]" . 0.75)
 ("link 61 [7 83]" . 0.0) ("link 62 [5 81]" . 0.75) ("link 63 [7 81]" . 0.0) ("link 64 [5 79]" . 0.75)
 ("link 65 [7 79]" . 0.0) ("link 66 [5 77]" . 0.75) ("link 67 [7 77]" . 0.0) ("link 68 [5 75]" . 0.75)
 ("link 69 [7 75]" . 0.0) ("link 70 [5 73]" . 0.75) ("link 71 [7 73]" . 0.0) ("link 72 [5 71]" . 0.75)
 ("link 73 [7 71]" . 0.0) ("link 74 [5 69]" . 0.75) ("link 75 [7 69]" . 0.0) ("link 76 [5 67]" . 0.75)
 ("link 77 [7 67]" . 0.0) ("link 78 [5 65]" . 0.75) ("link 79 [7 65]" . 0.0) ("link 80 [5 63]" . 0.75)
 ("link 81 [7 63]" . 0.0) ("link 82 [5 61]" . 0.75) ("link 83 [7 61]" . 0.0) ("link 84 [5 59]" . 0.75)
 ("link 85 [7 59]" . 0.0) ("link 86 [5 57]" . 0.75) ("link 87 [7 57]" . 0.0) ("link 88 [5 55]" . 0.75)
 ("link 89 [7 55]" . 0.0) ("link 90 [5 53]" . 0.75) ("link 91 [7 53]" . 0.0) ("link 92 [5 51]" . 0.75)
 ("link 93 [7 51]" . 0.0) ("link 94 [5 49]" . 0.75) ("link 95 [7 49]" . 0.0) ("link 96 [5 47]" . 0.75)
 ("link 97 [7 47]" . 0.0) ("link 98 [5 45]" . 0.75) ("link 99 [7 45]" . 0.0) ("link 100 [5 43]" . 0.75)
 ("link 101 [7 43]" . 0.0) ("link 102 [5 41]" . 0.75) ("link 103 [7 41]" . 0.0) ("link 104 [5 39]" . 0.75)
 ("link 105 [7 39]" . 0.0) ("link 106 [5 37]" . 0.75) ("link 107 [7 37]" . 0.0) ("link 108 [5 35]" . 0.75)
 ("link 109 [7 35]" . 0.0) ("link 110 [5 33]" . 0.75) ("link 111 [7 33]" . 0.0) ("link 112 [5 31]" . 0.75)
 ("link 113 [7 31]" . 0.0) ("link 114 [5 29]" . 0.75) ("link 115 [7 29]" . 0.0) ("link 116 [5 27]" . 0.75)
 ("link 117 [7 27]" . 0.0) ("link 118 [5 25]" . 0.75) ("link 119 [7 25]" . 0.0) ("link 120 [5 23]" . 0.75)
 ("link 121 [7 23]" . 0.0) ("link 122 [5 21]" . 0.75) ("link 123 [7 21]" . 0.0) ("link 124 [5 19]" . 0.75)
 ("link 125 [7 19]" . 0.0) ("link 126 [5 17]" . 0.75) ("link 127 [7 17]" . 0.0) ("link 128 [5 15]" . 0.75)
 ("link 129 [7 15]" . 0.0) ("link 130 [5 13]" . 0.75) ("link 131 [7 13]" . 0.0) ("link 132 [5 11]" . 0.75)
 ("link 133 [7 11]" . 0.0) ("link 134 [5 9]" . 0.75) ("link 135 [7 9]" . 0.0) ("link 136 [6 130]" . 0.5)
 ("link 137 [8 130]" . 0.0) ("link 138 [6 128]" . 0.5) ("link 139 [8 128]" . 0.0) ("link 140 [6 126]" . 0.5)
 ("link 141 [8 126]" . 0.0) ("link 142 [6 124]" . 0.5) ("link 143 [8 124]" . 0.0) ("link 144 [6 122]" . 0.5)
 ("link 145 [8 122]" . 0.0) ("link 146 [6 120]" . 0.5) ("link 147 [8 120]" . 0.0) ("link 148 [6 118]" . 0.5)
 ("link 149 [8 118]" . 0.0) ("link 150 [6 116]" . 0.5) ("link 151 [8 116]" . 0.0) ("link 152 [6 114]" . 0.5)
 ("link 153 [8 114]" . 0.0) ("link 154 [6 112]" . 0.5) ("link 155 [8 112]" . 0.0) ("link 156 [6 110]" . 0.5)
 ("link 157 [8 110]" . 0.0) ("link 158 [6 108]" . 0.5) ("link 159 [8 108]" . 0.0) ("link 160 [6 106]" . 0.5)
 ("link 161 [8 106]" . 0.0) ("link 162 [6 104]" . 0.5) ("link 163 [8 104]" . 0.0) ("link 164 [6 102]" . 0.5)
 ("link 165 [8 102]" . 0.0) ("link 166 [6 100]" . 0.5) ("link 167 [8 100]" . 0.0) ("link 168 [6 98]" . 0.5)
 ("link 169 [8 98]" . 0.0) ("link 170 [6 96]" . 0.5) ("link 171 [8 96]" . 0.0) ("link 172 [6 94]" . 0.5)
 ("link 173 [8 94]" . 0.0) ("link 174 [6 92]" . 0.5) ("link 175 [8 92]" . 0.0) ("link 176 [6 90]" . 0.5)
 ("link 177 [8 90]" . 0.0) ("link 178 [6 88]" . 0.5) ("link 179 [8 88]" . 0.0) ("link 180 [6 86]" . 0.5)
 ("link 181 [8 86]" . 0.0) ("link 182 [6 84]" . 0.5) ("link 183 [8 84]" . 0.0) ("link 184 [6 82]" . 0.5)
 ("link 185 [8 82]" . 0.0) ("link 186 [6 80]" . 0.5) ("link 187 [8 80]" . 0.0) ("link 188 [6 78]" . 0.5)
 ("link 189 [8 78]" . 0.0) ("link 190 [6 76]" . 0.5) ("link 191 [8 76]" . 0.0) ("link 192 [6 74]" . 0.5)
 ("link 193 [8 74]" . 0.0) ("link 194 [6 72]" . 0.5) ("link 195 [8 72]" . 0.0) ("link 196 [6 70]" . 0.5)
 ("link 197 [8 70]" . 0.0) ("link 198 [6 68]" . 0.5) ("link 199 [8 68]" . 0.0) ("link 200 [6 66]" . 0.5)
 ("link 201 [8 66]" . 0.0) ("link 202 [6 64]" . 0.5) ("link 203 [8 64]" . 0.0) ("link 204 [6 62]" . 0.5)
 ("link 205 [8 62]" . 0.0) ("link 206 [6 60]" . 0.5) ("link 207 [8 60]" . 0.0) ("link 208 [6 58]" . 0.5)
 ("link 209 [8 58]" . 0.0) ("link 210 [6 56]" . 0.5) ("link 211 [8 56]" . 0.0) ("link 212 [6 54]" . 0.5)
 ("link 213 [8 54]" . 0.0) ("link 214 [6 52]" . 0.5) ("link 215 [8 52]" . 0.0) ("link 216 [6 50]" . 0.5)
 ("link 217 [8 50]" . 0.0) ("link 218 [6 48]" . 0.5) ("link 219 [8 48]" . 0.0) ("link 220 [6 46]" . 0.5)
 ("link 221 [8 46]" . 0.0) ("link 222 [6 44]" . 0.5) ("link 223 [8 44]" . 0.0) ("link 224 [6 42]" . 0.5)
 ("link 225 [8 42]" . 0.0) ("link 226 [6 40]" . 0.5) ("link 227 [8 40]" . 0.0) ("link 228 [6 38]" . 0.5)
 ("link 229 [8 38]" . 0.0) ("link 230 [6 36]" . 0.5) ("link 231 [8 36]" . 0.0) ("link 232 [6 34]" . 0.5)
 ("link 233 [8 34]" . 0.0) ("link 234 [6 32]" . 0.5) ("link 235 [8 32]" . 0.0) ("link 236 [6 30]" . 0.5)
 ("link 237 [8 30]" . 0.0) ("link 238 [6 28]" . 0.5) ("link 239 [8 28]" . 0.0) ("link 240 [6 26]" . 0.5)
 ("link 241 [8 26]" . 0.0) ("link 242 [6 24]" . 0.5) ("link 243 [8 24]" . 0.0) ("link 244 [6 22]" . 0.5)
 ("link 245 [8 22]" . 0.0) ("link 246 [6 20]" . 0.5) ("link 247 [8 20]" . 0.0) ("link 248 [6 18]" . 0.5)
 ("link 249 [8 18]" . 0.0) ("link 250 [6 16]" . 0.5) ("link 251 [8 16]" . 0.0) ("link 252 [6 14]" . 0.5)
 ("link 253 [8 14]" . 0.0) ("link 254 [6 12]" . 0.5) ("link 255 [8 12]" . 0.0) ("link 256 [6 10]" . 0.5)
 ("link 257 [8 10]" . 0.0))

**** 結果 2
(("link 2 [1 2]" . 0.0) ("link 3 [3 4]" . 0.0) ("link 4 [5 7]" . 0.0) ("link 5 [6 8]" . 0.0)
 ("link 6 [1 3]" . 0.39104194416697147) ("link 7 [2 4]" . 0.0) ("link 8 [3 5]" . 0.2578288712853927)
 ("link 9 [4 7]" . 0.0) ("link 10 [3 6]" . 0.19033363447262022) ("link 11 [4 8]" . 0.0)
 ("link 12 [5 131]" . 0.11493875755970569) ("link 13 [7 131]" . 0.0) ("link 14 [5 129]" . 0.12465835060779895)
 ("link 15 [7 129]" . 0.0) ("link 16 [5 127]" . 0.10213880972195681) ("link 17 [7 127]" . 0.0)
 ("link 18 [5 125]" . 0.12041667877572) ("link 19 [7 125]" . 0.0) ("link 20 [5 123]" . 0.1272257840660417)
 ("link 21 [7 123]" . 0.0) ("link 22 [5 121]" . 0.08465980470388307) ("link 23 [7 121]" . 0.0)
 ("link 24 [5 119]" . 0.0900292961152979) ("link 25 [7 119]" . 0.0) ("link 26 [5 117]" . 0.07451670321709879)
 ("link 27 [7 117]" . 0.0) ("link 28 [5 115]" . 0.0952827915822631) ("link 29 [7 115]" . 0.0)
 ("link 30 [5 113]" . 0.11520019788903187) ("link 31 [7 113]" . 0.0) ("link 32 [5 111]" . 0.11165878638934092)
 ("link 33 [7 111]" . 0.0) ("link 34 [5 109]" . 0.08877111321832572) ("link 35 [7 109]" . 0.0)
 ("link 36 [5 107]" . 0.11913369436632903) ("link 37 [7 107]" . 0.0) ("link 38 [5 105]" . 0.08727051277487341)
 ("link 39 [7 105]" . 0.0) ("link 40 [5 103]" . 0.07626721310011901) ("link 41 [7 103]" . 0.0)
 ("link 42 [5 101]" . 0.075781659054323) ("link 43 [7 101]" . 0.0) ("link 44 [5 99]" . 0.0825872201878602)
 ("link 45 [7 99]" . 0.0) ("link 46 [5 97]" . 0.11192096181624682) ("link 47 [7 97]" . 0.0)
 ("link 48 [5 95]" . 0.12331077116725127) ("link 49 [7 95]" . 0.0) ("link 50 [5 93]" . 0.10249291392946629)
 ("link 51 [7 93]" . 0.0) ("link 52 [5 91]" . 0.05731648795643109) ("link 53 [7 91]" . 0.0)
 ("link 54 [5 89]" . 0.10073336707286602) ("link 55 [7 89]" . 0.0) ("link 56 [5 87]" . 0.05449417713771754)
 ("link 57 [7 87]" . 0.0) ("link 58 [5 85]" . 0.10033994124833383) ("link 59 [7 85]" . 0.0)
 ("link 60 [5 83]" . 0.0689481944113835) ("link 61 [7 83]" . 0.0) ("link 62 [5 81]" . 0.11466411136391123)
 ("link 63 [7 81]" . 0.0) ("link 64 [5 79]" . 0.09413774603634217) ("link 65 [7 79]" . 0.0)
 ("link 66 [5 77]" . 0.09900465878483715) ("link 67 [7 77]" . 0.0) ("link 68 [5 75]" . 0.0955478037193898)
 ("link 69 [7 75]" . 0.0) ("link 70 [5 73]" . 0.10909993337250982) ("link 71 [7 73]" . 0.0)
 ("link 72 [5 71]" . 0.12863065981238228) ("link 73 [7 71]" . 0.0) ("link 74 [5 69]" . 0.12611566596791798)
 ("link 75 [7 69]" . 0.0) ("link 76 [5 67]" . 0.12616972344067415) ("link 77 [7 67]" . 0.0)
 ("link 78 [5 65]" . 0.12170063143465044) ("link 79 [7 65]" . 0.0) ("link 80 [5 63]" . 0.11472722179818977)
 ("link 81 [7 63]" . 0.0) ("link 82 [5 61]" . 0.07520234169962166) ("link 83 [7 61]" . 0.0)
 ("link 84 [5 59]" . 0.11726311700281056) ("link 85 [7 59]" . 0.0) ("link 86 [5 57]" . 0.14053139703478276)
 ("link 87 [7 57]" . 0.0) ("link 88 [5 55]" . 0.11858461899839327) ("link 89 [7 55]" . 0.0)
 ("link 90 [5 53]" . 0.1283864531051348) ("link 91 [7 53]" . 0.0) ("link 92 [5 51]" . 0.12558610428473138)
 ("link 93 [7 51]" . 0.0) ("link 94 [5 49]" . 0.10303660629271387) ("link 95 [7 49]" . 0.0)
 ("link 96 [5 47]" . 0.06159717715771642) ("link 97 [7 47]" . 0.0) ("link 98 [5 45]" . 0.08836214352371806)
 ("link 99 [7 45]" . 0.0) ("link 100 [5 43]" . 0.10084611228333339) ("link 101 [7 43]" . 0.0)
 ("link 102 [5 41]" . 0.09072741011898415) ("link 103 [7 41]" . 0.0) ("link 104 [5 39]" . 0.12303112376005328)
 ("link 105 [7 39]" . 0.0) ("link 106 [5 37]" . 0.11048792884036869) ("link 107 [7 37]" . 0.0)
 ("link 108 [5 35]" . 0.07787373129157125) ("link 109 [7 35]" . 0.0) ("link 110 [5 33]" . 0.13444855968141636)
 ("link 111 [7 33]" . 0.0) ("link 112 [5 31]" . 0.06506593021058647) ("link 113 [7 31]" . 0.0)
 ("link 114 [5 29]" . 0.12800979649550784) ("link 115 [7 29]" . 0.0) ("link 116 [5 27]" . 0.11863658628158964)
 ("link 117 [7 27]" . 0.0) ("link 118 [5 25]" . 0.12081363059827777) ("link 119 [7 25]" . 0.0)
 ("link 120 [5 23]" . 0.1296251001804581) ("link 121 [7 23]" . 0.0) ("link 122 [5 21]" . 0.11426191355956716)
 ("link 123 [7 21]" . 0.0) ("link 124 [5 19]" . 0.09740018247593414) ("link 125 [7 19]" . 0.0)
 ("link 126 [5 17]" . 0.11253402485917414) ("link 127 [7 17]" . 0.0) ("link 128 [5 15]" . 0.08780009309971547)
 ("link 129 [7 15]" . 0.0) ("link 130 [5 13]" . 0.07543951884720625) ("link 131 [7 13]" . 0.0)
 ("link 132 [5 11]" . 0.09451822235851354) ("link 133 [7 11]" . 0.0) ("link 134 [5 9]" . 0.1295414737972765)
 ("link 135 [7 9]" . 0.0) ("link 136 [6 130]" . 0.11658560931233519) ("link 137 [8 130]" . 0.0)
 ("link 138 [6 128]" . 0.11428304403397715) ("link 139 [8 128]" . 0.0)
 ("link 140 [6 126]" . 0.13108470844305284) ("link 141 [8 126]" . 0.0)
 ("link 142 [6 124]" . 0.14220305421115895) ("link 143 [8 124]" . 0.0)
 ("link 144 [6 122]" . 0.10844781489255485) ("link 145 [8 122]" . 0.0)
 ("link 146 [6 120]" . 0.13517771491735972) ("link 147 [8 120]" . 0.0)
 ("link 148 [6 118]" . 0.13226724612437304) ("link 149 [8 118]" . 0.0)
 ("link 150 [6 116]" . 0.14730302546054827) ("link 151 [8 116]" . 0.0) ("link 152 [6 114]" . 0.1007058305233532)
 ("link 153 [8 114]" . 0.0) ("link 154 [6 112]" . 0.13199062258130914) ("link 155 [8 112]" . 0.0)
 ("link 156 [6 110]" . 0.12484520865188625) ("link 157 [8 110]" . 0.0)
 ("link 158 [6 108]" . 0.13945181504074847) ("link 159 [8 108]" . 0.0)
 ("link 160 [6 106]" . 0.11747650184873454) ("link 161 [8 106]" . 0.0)
 ("link 162 [6 104]" . 0.13480248834622444) ("link 163 [8 104]" . 0.0)
 ("link 164 [6 102]" . 0.08792834634011543) ("link 165 [8 102]" . 0.0) ("link 166 [6 100]" . 0.1417494924540446)
 ("link 167 [8 100]" . 0.0) ("link 168 [6 98]" . 0.06269940386181284) ("link 169 [8 98]" . 0.0)
 ("link 170 [6 96]" . 0.13200369180481866) ("link 171 [8 96]" . 0.0) ("link 172 [6 94]" . 0.09865342351519253)
 ("link 173 [8 94]" . 0.0) ("link 174 [6 92]" . 0.07774205856886447) ("link 175 [8 92]" . 0.0)
 ("link 176 [6 90]" . 0.11399298052800999) ("link 177 [8 90]" . 0.0) ("link 178 [6 88]" . 0.12834545267077827)
 ("link 179 [8 88]" . 0.0) ("link 180 [6 86]" . 0.11058524396461282) ("link 181 [8 86]" . 0.0)
 ("link 182 [6 84]" . 0.1268601798493306) ("link 183 [8 84]" . 0.0) ("link 184 [6 82]" . 0.07880430737915532)
 ("link 185 [8 82]" . 0.0) ("link 186 [6 80]" . 0.14715974480040303) ("link 187 [8 80]" . 0.0)
 ("link 188 [6 78]" . 0.09849669216255878) ("link 189 [8 78]" . 0.0) ("link 190 [6 76]" . 0.12847185201308595)
 ("link 191 [8 76]" . 0.0) ("link 192 [6 74]" . 0.09236860641560458) ("link 193 [8 74]" . 0.0)
 ("link 194 [6 72]" . 0.09317010956527404) ("link 195 [8 72]" . 0.0) ("link 196 [6 70]" . 0.10120373028401757)
 ("link 197 [8 70]" . 0.0) ("link 198 [6 68]" . 0.1152480765020209) ("link 199 [8 68]" . 0.0)
 ("link 200 [6 66]" . 0.15036840681910077) ("link 201 [8 66]" . 0.0) ("link 202 [6 64]" . 0.12190090408348235)
 ("link 203 [8 64]" . 0.0) ("link 204 [6 62]" . 0.10768529774938862) ("link 205 [8 62]" . 0.0)
 ("link 206 [6 60]" . 0.0873658402620457) ("link 207 [8 60]" . 0.0) ("link 208 [6 58]" . 0.12466135204206802)
 ("link 209 [8 58]" . 0.0) ("link 210 [6 56]" . 0.12810260048602762) ("link 211 [8 56]" . 0.0)
 ("link 212 [6 54]" . 0.12313629825863194) ("link 213 [8 54]" . 0.0) ("link 214 [6 52]" . 0.09008205385569429)
 ("link 215 [8 52]" . 0.0) ("link 216 [6 50]" . 0.04246034272954694) ("link 217 [8 50]" . 0.0)
 ("link 218 [6 48]" . 0.047899385427360074) ("link 219 [8 48]" . 0.0) ("link 220 [6 46]" . 0.09843455283296591)
 ("link 221 [8 46]" . 0.0) ("link 222 [6 44]" . 0.11893117857386672) ("link 223 [8 44]" . 0.0)
 ("link 224 [6 42]" . 0.1448495262895705) ("link 225 [8 42]" . 0.0) ("link 226 [6 40]" . 0.08895902681873302)
 ("link 227 [8 40]" . 0.0) ("link 228 [6 38]" . 0.11058763647023273) ("link 229 [8 38]" . 0.0)
 ("link 230 [6 36]" . 0.12993055155926783) ("link 231 [8 36]" . 0.0) ("link 232 [6 34]" . 0.08336687829866003)
 ("link 233 [8 34]" . 0.0) ("link 234 [6 32]" . 0.1268083004190256) ("link 235 [8 32]" . 0.0)
 ("link 236 [6 30]" . 0.09228310097739569) ("link 237 [8 30]" . 0.0) ("link 238 [6 28]" . 0.0991396121515566)
 ("link 239 [8 28]" . 0.0) ("link 240 [6 26]" . 0.1064691177131592) ("link 241 [8 26]" . 0.0)
 ("link 242 [6 24]" . 0.09337561958537988) ("link 243 [8 24]" . 0.0) ("link 244 [6 22]" . 0.11913203900234069)
 ("link 245 [8 22]" . 0.0) ("link 246 [6 20]" . 0.11212243155402758) ("link 247 [8 20]" . 0.0)
 ("link 248 [6 18]" . 0.087804886654269) ("link 249 [8 18]" . 0.0) ("link 250 [6 16]" . 0.10696260518149452)
 ("link 251 [8 16]" . 0.0) ("link 252 [6 14]" . 0.09677324791852408) ("link 253 [8 14]" . 0.0)
 ("link 254 [6 12]" . 0.09733705864620112) ("link 255 [8 12]" . 0.0) ("link 256 [6 10]" . 0.06243641170210309)
 ("link 257 [8 10]" . 0.0))

* ルーター故障診断
** モチベーション
SNMP によって得られたルーターのデータ

- CPU 利用状況
- Memory 利用状況
- Traffic 情報

この三つのパラメータから、ルーターの異常を検知したい。
*** 留意
- 故障かそうでないかの判定データは得られない。よって基本的には教師なし学習。
- 監視中に状態が変わることも考えられる。よって強化学習的な枠組みも必要？
** 草案
三つのパラメータには何らかの関係があることを基本に考え、関係の崩れを見出したい。

- Traffic が増大すれば、CPU Memory 利用率も上がる。
- ルータの製造元によって、相関にはくせがある。

など。それを機械学習手法で抽出し、正常時と異常時が区別できないか。
*** 各パラメータの正常値
- 自動車故障診断技術の応用
各ルータにはいくつか状態（ traffic が流れている／ない など )
があって各パラメータは状態に応じて正常値をもつと考える。

クラスタリングを用いて各状態ごとにデータがまとめられれば、
各クラスタで典型的な各パラメータの値を正常値と考え、
機械はルータに対する判断基準をもち、異常検知ができる。

注意しなければならないのは、各クラスタが異常状態でまとまらないようにすることである。
上記３パラメータから察するにルーターが置かれた状況は Traffic 情報で
分類でき、かつ Traffic 情報はルータの故障を表現する指標ではないと考えられる。
よって Traffic 情報になんらかの特徴素を定義し、
（特徴素例：各統計量、傾き（上昇／下降傾向）、周期（0 1 2 3 ...)、）
それを元にクラスタリングを行うのがよいと考えられる。

特徴素が離散的な値でいくつか表現できれば、
クラスタリングの手法として LDA を用いることができる。
LDA は 自然言語処理分野の技術で、従来手法よりも優れた文脈抽出が行える上、
クラスタ数 k の決定も最適なものを自動で行うことができる。
音声や画像認識にも使われており、応用分野は広い。
*** 時系列解析的手法
**** 多変量時系列モデル
多変量時系列モデルを用いて、３パラメータ時系列データのモデルを
構築することで複合的な予測値や、各パラメータ間の関連を解析する。

- VARモデル(多変量 AR モデル)
例: 船舶の航行データとして 方向角速度／縦揺れ／舵角 の三つのパラメータを定義し、
    VAR モデルを構成しクロススペクトル(相互相関関数のスペクトル)
    を用いて、各パラメータの関連を解析、オートパイロット時の特徴を数値化した。

予測値との外れ具合を評価して異常を検知することも考えられる。
**** CCF
時系列解析における相互相関係数( Cross Correlation Function )を用いることで、
各パラメータ間の相関を時間のずれを考慮して抽出することができる。

**** スイッチング状態空間モデル
時系列データ解析で頻繁に用いられる状態空間モデルは、
時系列が定常状態にあれば精度のよいモデルである。
しかし状態の急激な変化に（社会的事件による株価の変動など）
対応することはできない。

状態の変化点を含んだ時系列モデルとしてスイッチング状態空間モデルがある。
スイッチング状態空間モデルは、状態の記述に隠れマルコフモデルを用い、
各状態に応じた状態空間モデルによってフィッティングするモデルである。

この方法を用いれば、ルーターの状態変化にも対応できる。
**** カオス時系列解析
従来の、なんらかの周期を見出そうとする方法、の先。
周期のない一見カオスなデータからもなんらかの規則性が見出せる。
（フラクタル解析？）
http://www.nls.ics.saitama-u.ac.jp/~tohru/LaTeX/.../kobe_intro.pdf

それ自体は単変量に注目した考え方だが、規則性の相関があれば見出せないだろうか。
* CiNii log 解析
** CINII log を LDA 分析することのメリットについて
「 検索クエリの流行トピックおよび各トピックにおける流行検索単語抽出 」
*** 概要:
現在の検索エンジンでは、検索クエリとして「的確に欲しい情報を持ってこれる語」を
入力する必要があるが、語句が多義的／汎用的であることなどにより、検索結果が「的確な欲しい情報」
を持ってこれない場合が多々ある。しかし多くのユーザーは流行している分野の注目されている論文を
参照したいはずであり、すぐにそういった論文にたどりつけないのはもどかしいことである。

CINIIの検索ログからは、ユーザーが入力した検索クエリおよびその検索結果からどの論文を選んだかの
情報を得ることができ、これは「検索クエリに対する的確な欲しい情報（論文）」の集まりと考えられる。

自然言語処理における潜在的意味解析(Latent Semantic Analysis)の技術を用いて、
このログを解析すれば、検索クエリと論文の間に潜在する意味（トピック）すなわち
検索におけるユーザーの文脈（"自然言語処理" など）を捉え、
トピックの流行および各トピックにおける流行検索語を抽出することができる。

これら流行トピックおよび各トピックで流行している検索語句を
検索サイトのトップに表示することで、ユーザーは検索語句を入力することなく、
流行トピック／流行単語をたどることで今注目されている論文にたどりつくことができる。

このような潜在的意味解析の手法の一つとして LDA(潜在ディリクレ配分法) がある。
*** LDA の従来手法に対する利点
潜在的意味解析の研究において、文書を単語のベクトルで表現したときに超高次元になることが
一つの大きな問題であり、いかにしてこの次元を減らしつつ、
ベクトルを文書の特徴量として利用するアプリケーションの性能を落とさないかという課題がある。
LDA はそれに対処するための次元圧縮のための文書モデルの一つである。
他にも同様な文書モデル／次元圧縮手法はあり

PCA(主成分分析)
LSI(潜在的意味インデクシング)
pLSI(確率的潜在的意味インデクシング)

などが考えられてきた。
LDA は pLSI をベイズ統計の観点からより洗練したものであるが、
これらの手法より LDA は
パープレキシティ(文書モデルの評価指標、真のモデルからのずれを表す)
が低くモデルとして強力だとされており
実際に
文書クラスタリングを用いた各手法の比較
(1. 正田備也、喜安千弥、宮原末治 "文書クラスタリングのためのCollapsed変分ベイズLDAによる次元圧縮")
において、
- LDA, pLSI は PCA より次元圧縮後のベクトルの精度が高い。
  すなわち文書に潜在する意味をより適切に捉えている。
- 一般に次元を減らせばそれだけ情報量が落ちるが、
  LDA, pLSI は 文脈を捉えるという意味では、
  次元圧縮以前とほぼ同じ質の文書ベクトル（低次元）を実現できている。
ことや、

科学分野の文書集合に LDA を施し
(2. Griffith, Steyvers "Finding scientific topics")
流行トピックおよび各トピックの主たる単語が抽出できること、
などの報告がある。

pLSI と LDA はあまり精度の差がないようではあるが、
それは pLSI のモデルチューニングを上手くやれた上での話である。
すなわち pLSI に対する LDA の利点として

- 両方ともトピック数を指定しなければならないモデルである。しかし、
  hierarchical Dirichlet Process (hDP) という手法が近年提案されており
  LDA のアルゴリズムとして hDP を適用することで、最適なトピック数も推定できる。
- pLSI では各文書のトピックに関する事前知識が必要であるが、
  LDA では各文書のトピック分布も確率的に推定するため事前知識は不要であり、
  過学習が起きにくいロバストな文書モデルを実現している。

と、LDA は発見的方法に頼らず、自動で高精度な次元圧縮／潜在意味解析が行える。

*** 例
データ: cinii における検索ログ(2009/6/1 - 6/7)
上記データに対して、トピック数 k = 20 としてトピックを抽出し、
各トピック（単語の集合）を流行順にならべたものが以下である。
各トピック内の単語はそのトピック内での流行順になっている。

((7 . #("Plant" "児童" "湿気" "効果" "心身医学" ...))
 (4 . #("土と基礎" "農村計画," "Ecology" "環境工学II," "Landscape" ...))
 (9 . #("II" "mathematics" "片脚立位" "精密機械" "ミューラー" ...))
 (5 . #("心身医学" "Nippon"  "Letters" "農村計画," "Physical" ...))
 (14 . #("食品" "日本数学教育学会誌" "土と基礎" "色彩" "Anomalies" ...))
 (15 . #("高齢者の住まい" "遊び" "P2P" "観光" "コンピュータソフトウェア" ...))
 (13 . #("意識" "グローバル化" "大学生" "治療" "ドラフト制度" ...))
 (19 . #("視線" "情報の科学と技術" "食事調査" "日本消化器外科学会雑誌" "下駄" ...))
 (8 . #("Letters" "自然エネルギー" "Progress" "仕事" "評価" ...))
 (18 . #("尺度" "proceedings" "栄養" "アメリカ" "ユーモア志向" ...)) ...)

以上から例えば、 1 番に流行しているトピック 7 の 2 番目に流行している単語 "児童"
を選んだとする。まずトピック 7 に該当する文書ID は以下である。

(40005412170 110004045970 110004015097 110002102930 10009957910 110003199698 110006570166 110003287599
 110002755995 110001912305 ...)

さらにこれら文書の中で単語 "児童" をこのトピックの文脈で多く含むものを上位に配置し
検索結果として表示する。

文書ID: 40016363462, キーワード: #(心理 児童)
文書ID: 110004713574, キーワード: #(心理 児童)
文書ID: 110006162454, キーワード: #(電子情報通信学会技術研究報告 情報ネットワーク IN)
文書ID: 40007222611, キーワード: #(人文 社会科学編)
文書ID: 10021371302, キーワード: #(学習 プログラミング 論文誌)
文書ID: 110002818575, キーワード: #(リハビリテーション医学 : 日本リハビリテーション医学会誌)
文書ID: 110004690466, キーワード: #(: 看護と情報 看護図書館協議会会誌)
文書ID: 40016369328, キーワード: #(物流 中国)
文書ID: 110006162915, キーワード: #(the Society Circulation Circulation of : official journal journal Japanese)
文書ID: 110002063723, キーワード: #(The of bulletin Dental)
文書ID: 10019841367, キーワード: #(北海道教育大学紀要 社会科学編 人文科学)
文書ID: 110006251480, キーワード: #(リハビリテーション医学 : 日本リハビリテーション医学会誌)
文書ID: 110004587067, キーワード: #(reports data JARE)
文書ID: 40005085830, キーワード: #(リハビリテーション医学 : 日本リハビリテーション医学会誌)
文書ID: 110007055581, キーワード: #(the Society of of and Japan Palaeontological proceedings New series Transactions)
...

この結果から 2009年6月の第一週では、児童心理学に関する二つの論文が注目されており、
児童心理学に興味のあるユーザーはホットな論文にすぐにたどり着けることがわかる。

今度は "農村計画," という単語に着目してみる。
トピック 4 と 5 で主たる検索語句として抽出されているが、
4 は "環境"、5 は "心身" と、それぞれ潜在する意味が類推できる。
それぞれのトピックで "農村計画," の検索結果を、同様に表示してみると

4 では
文書ID: 120000827871, キーワード: #(農村計画, 土と基礎)
文書ID: 110004597513, キーワード: #(農村計画, Ecology)
文書ID: 110004657664, キーワード: #(農村計画,)
文書ID: 110007068137, キーワード: #(吸着 土 ヒ素)
文書ID: 40004555447, キーワード: #(dioxin 比較 下駄)
...

5 では
文書ID: 40005581217, キーワード: #(農村計画, 心身医学)
文書ID: 40004515907, キーワード: #(農村計画, physical)
文書ID: 110006426932, キーワード: #(Asia 中国 常正)
文書ID: 40016003663, キーワード: #(流通 小売 韓国)
文書ID: 110006794723, キーワード: #(学生 看護 日本)
...

となり、それぞれ違ったトピックにおける主たる文書が検索結果として表示される。

*** 検索クエリをトピック分類することの利点
- トピックによる文書クラスタリング
抽出したトピックを基準に、データベース上にある論文をクラスタリングすることにより
検索はされていないが、そのトピックに属すと思われる論文を拾うことができ、
それらをトピックによる検索結果に表示することで、流行している論文とともに
流行はしていないが、そのトピックに属すと思われる論文を提示することができる。

- 多重トピック(TopicA + TopicB) による流行論文検索
LDA では各文書が複数のトピックにまたがったものであることも表現できる。
このことを利用すれば、
ユーザーにとって、どのトピックもぴったりではないがいくつか複数のトピックに
要求と合致する部分を感じる、などといった場合、
複数のトピックを選ぶことで、各トピックにまたがった内容の論文も
検索結果として例に示したように表示できる。

- トピックの時系列解析
分析対象データをある時区間で区切ることで、
各トピックの流行度合いは時系列データとして扱える。
そうすれば流行り廃りの周期性やトピック間の時系列的な相関を解析
することができる。

*** 問題点
- トピック数 k の推定
  LDA ではトピック数 k として妥当なものをあらかじめ指定しなければならない。
  上例の k = 20 は明らかにトピックを分類できていない。
  - hierarchical Dirichlet Process (hDP) を LDA に導入することで k も推定できる。
    k を手動で変化させた最も好ましい結果と hDP-LDA は性能として同じとの報告あり。

- 検索クエリのノイズ
  "the" や ":" などの、トピックを弁別する能力が明らかに低いが頻度が高い語は、解析のノイズとなってしまう。
  - 単語の重み付け( tf-idf 法など)手法や、あらかじめ設定したノイズ語によって切り捨てる。

- トピック名
  各トピックは単語の集合であり、名前がない。
  名前付けは単語の集合を見て、人手で行う必要がある。
  - 既存のカテゴリー、例えば 代数学、生態学 ... を選択肢として提示し、当てはめる方式をイメージしている。
    もし既存のカテゴリーにない場合は、手入力で新しいカテゴリーを生成。
    - 新分野または各分野にまたがるような内容のカテゴリーを発見できる可能性がある。

- 分析対象期間
  どれくらいのスパンで流行トピック抽出を行うか。
  上例では一週間分のデータを用いており、文書数は 1173 件である。
  どのくらいの規模のデータが適切かについての議論はあまり成されていないため、
  調査が必要である。

** HDP-LDA による解析結果例
流行順にならべた各トピック

((4  . #(":" "リハビリテーション医学" "日本リハビリテーション医学会誌" "日本気管支研究会雑誌" "気管支学" "鐵と鋼" "日本鐡鋼協會々誌" "映像情報メディア学会誌" "映像情報メディア"
      "日本循環器学会専門医誌"))
 (8 . #("journal" "Japanese" "circulation" "of" "Circulation" ":" "the" "The" "Society" "official"))
 (22 . #("of" "Journal" "and" "the" "Japan" "Society" "research" "science" "Bulletin" "University"))
 (0 . #("bulletin" "&" "pharmaceutical" "Chemical" "Biological" "journal" "F-2" "オペレーションズ" "臨時増刊" "環境管理"))
 (12 . #("and" "on" "Soil" "IEICE" "biotechnology" "biochemistry" "zoology" "entomology" "transactions" "of"))
 (2 . #("電子情報通信学会技術研究報告" "情報処理学会研究報告" "JSME" "学習" "A" "論文誌" "プログラミング" "グローバル" "meeting" "年次大会講演論文集"))
 (20 . #("学術講演梗概集" "電子情報通信学会論文誌" "B" "構造II" "材料施工" "C" "構造I" "A" "海洋" "A-1"))
 (3 . #(":" "D" "&" "ronbunshi" "日本セラミックス協会学術論文誌" "Seramikkusu" "Nippon" "Design" "Conference" "gakujutsu"))
 (14 . #("学術講演梗概集" "都市計画" "建築経済" "住宅問題" "建築歴史" "意匠" "F" "F-1" "環境工学" "研究報告集"))
 (5 . #("学術講演梗概集" "B-1" "信頼性" "膜構造" "シェル" "応用力学" "構造解析" "構造I" "立体構造" "基礎構造")) ...)

トピック数は 47, だいたい 50 近辺に収束する。
乱数をあちこちで使用するので、50 のあたりでの細かな振動は不可避。
単なる LDA の結果よりも良くまとまっている。

"学術講演梗概集" という単語がトピック 20, 14, 5 で最大確率の単語となっており、
それぞれトピックも違うようである。(トピック類推、20:電子 14:建築 5:物性物理/構造)
三つのトピックで単語を "学術講演梗概集" として文書を絞ったところ

- topic 20
文書ID: 110002396445, キーワード: (構造I 学術講演梗概集 B)
文書ID: 110002888944, キーワード: (構造II B-2 原子力プラント 学術講演梗概集 振動)
文書ID: 110006570166, キーワード: (学術講演梗概集)
文書ID: 110004048807, キーワード: (A-1 材料施工 学術講演梗概集)
文書ID: 110006885392, キーワード: (構造II C 学術講演梗概集)
文書ID: 110007005729, キーワード: (D 学術講演梗概集 環境工学)
文書ID: 110001993243, キーワード: (情報システム技術 学術講演梗概集 海洋 防火 A-2)
文書ID: 110002491705, キーワード: (情報システム技術 材料施工 A 学術講演梗概集 海洋 防火)
文書ID: 110006337596, キーワード: (構造I 学術講演梗概集 B)
文書ID: 110006560654, キーワード: (構造I 学術講演梗概集 B)
...

- topic 14
文書ID: 10019765276, キーワード: (住宅問題 都市計画 建築経済 学術講演梗概集 F-1)
文書ID: 20000829959, キーワード: (住宅問題 都市計画 建築経済 学術講演梗概集 F-1)
文書ID: 40015714196, キーワード: (住宅問題 都市計画 建築経済 学術講演梗概集 F-1)
文書ID: 110004174522, キーワード: (住宅問題 都市計画 建築経済 F 学術講演梗概集)
文書ID: 110004779594, キーワード: (住宅問題 都市計画 建築経済 学術講演梗概集 F-1)
文書ID: 110004145700, キーワード: (住宅問題 都市計画 建築経済 学術講演梗概集 F-1)
文書ID: 110002755181, キーワード: (構造II C 学術講演梗概集)
文書ID: 110006975340, キーワード: (住宅問題 都市計画 建築歴史 建築経済 F 意匠 学術講演梗概集)
文書ID: 110002416498, キーワード: (施工 材料 A 学術講演梗概集 海洋 防火)
文書ID: 40016121882, キーワード: (D 学術講演梗概集 環境工学)
...

- topic 5
文書ID: 110002224465, キーワード: (立体構造 B-1 信頼性 膜構造 シェル 基礎構造 荷重 構造I 学術講演梗概集 応用力学 構造解析)
文書ID: 110006283610, キーワード: (立体構造 B-1 信頼性 膜構造 シェル 基礎構造 荷重 構造I 学術講演梗概集 応用力学 構造解析)
文書ID: 110004303164, キーワード: (立体構造 B-1 信頼性 膜構造 シェル 基礎構造 荷重 構造I 学術講演梗概集 応用力学 構造解析)
文書ID: 110004221457, キーワード: (立体構造 B-1 信頼性 膜構造 シェル 基礎構造 荷重 構造I 学術講演梗概集 応用力学 構造解析)
文書ID: 110001143290, キーワード: (立体構造 B-1 信頼性 膜構造 シェル 基礎構造 荷重 構造I 学術講演梗概集 応用力学 構造解析)
文書ID: 110002376126, キーワード: (立体構造 B-1 信頼性 膜構造 シェル 基礎構造 荷重 構造I 学術講演梗概集 応用力学 構造解析)
文書ID: 110003884627, キーワード: (立体構造 B-1 信頼性 膜構造 シェル 基礎構造 荷重 構造I 学術講演梗概集 応用力学 構造解析)
文書ID: 40006284893, キーワード: (ファイナンス 地方 再生)
文書ID: 110000955306, キーワード: (色 食品)
文書ID: 110006671188, キーワード: (立体構造 B-1 信頼性 膜構造 シェル 基礎構造 荷重 構造I 学術講演梗概集 応用力学 構造解析)
...

となった。各トピックに対応する文書が当たっているように思われる。
5 における 10 位の文書は "学術講演梗概集" が入っているが上位に来ないのは、
"学術講演梗概集" がトピック 5 ではなく、20 の文脈であると判断されていたためである。
トピックを構成する単語を見てみると、たしかに 5 と 20 は近いトピックのようである。

* 作業メモ
** 主成分分析応用
主成分分析を用いた例として、固有顔・部分空間法などの顔認識手法があり、それらを実装した。
一般には条件さえ整えば固有顔法などは高い判別率(90%以上)を示すことが知られている。

現状のMLにある固有顔法で、サンプルデータ eyes200.sexp(9個人、のべ200人分、画素数 60*28 = 1680)を
学習用 100, 検証用 100 に分割し、その正解率を算出したところ以下のようになった。
分割はランダムに行い、各次元数に対して 10 回検証を行なった。

次元数: 5, 正解率(min max mean): 0.73 0.86 0.81
次元数: 10, 正解率(min max mean): 0.78 0.89 0.83
次元数: 15, 正解率(min max mean): 0.73 0.91 0.83
次元数: 20, 正解率(min max mean): 0.80 0.89 0.84

サンプルデータが固有顔法のために作成したものでない(右目のみ、明暗を統一していない、人物の大きさが違う)
ことを差し引いて考えれば、正解率は期待通りではないだろうか。
明暗が統一されていないデータに対する方法として、各顔データの正規化がある。
人物に対して光源の位置が一定と仮定すれば、
あるパターンに対しての明暗の違いは単に輝度値の大小の違いであり、輝度値のパターンは同じであるので、
平均が 0 になるように各顔データを正規化してやれば、明暗の違いを吸収できる。
eyes200.sexp には ”光源の位置が一定” などという前提はないが、
平均が 0 になるように正規化して同様に検証を行なったところ以下のようになった。

次元数: 5, 正解率(min max mean): 0.81 0.87 0.84
次元数: 10, 正解率(min max mean): 0.78 0.89 0.86
次元数: 15, 正解率(min max mean): 0.79 0.91 0.85
次元数: 20, 正解率(min max mean): 0.85 0.93 0.90

正解率は軒並み上昇している。
さらに、サンプルデータを右目のみだけでなく、両目に増やしたらどうなるか、
両目のサンプルデータ deyes200.sexp (画素数 115 * 26 = 2990) を平均が 0 になるように正規化し、
画素数が増えたので、次元数も少し伸ばして同様に検証したところ、

次元数: 5, 正解率(min max mean): 0.74 0.91 0.83
次元数: 10, 正解率(min max mean): 0.86 0.91 0.89
次元数: 15, 正解率(min max mean): 0.86 0.92 0.89
次元数: 20, 正解率(min max mean): 0.87 0.94 0.91
次元数: 25, 正解率(min max mean): 0.85 0.95 0.90
次元数: 30, 正解率(min max mean): 0.83 0.95 0.90
次元数: 35, 正解率(min max mean): 0.88 0.97 0.92

と、9 割以上の正解率が多くを占めるようになった。
さらに多くの画素、顔全体(画素数 140 * 140 = 19600)で行なえばさらに高い正解率が期待できるかもしれないが、
現状の実装では STORAGE-CONDITION となってしまうので、さらなる工夫／何か別の方法が必要である。

一つの方法として固有顔法にモデルの結合(committee)の手法を適用させてやることが考えられる。
以下はバギング(bagging)というモデル結合手法を、サンプルデータ数２０として固有顔法に適用させて、
両目のデータ deyes200.sexp で正解率を検証した結果である。

次元数: 5, 正解率(min max mean): 0.79 0.90 0.84
次元数: 10, 正解率(min max mean): 0.80 0.93 0.87
次元数: 15, 正解率(min max mean): 0.87 0.96 0.91
次元数: 20, 正解率(min max mean): 0.85 0.93 0.89
次元数: 25, 正解率(min max mean): 0.87 0.94 0.90
次元数: 30, 正解率(min max mean): 0.83 0.95 0.92
次元数: 35, 正解率(min max mean): 0.85 0.96 0.90

明らかな寄与は見られない。モデル結合は一般には弱学習器（ランダムよりはちょっといいくらい）のものを
集めて高い精度を実現するものであるが、固有顔法はそもそも精度が高いので、単に推定結果を攪乱するだけで、
あまり改善はされないと考えられる。

よく知られた主成分分析を用いた顔パターン認識手法は他にもあり
- 部分空間法
  - 人物の姿勢変動に強いと言われている。だが現状の実装・サンプルデータでは期待の精度はでておらず、目下調査中。
    サンプルデータは特に姿勢変動という要素はあまりないが、少なくとも固有顔法程度の精度は期待できるはず。
- 相互部分空間法
- 核非線形相互部分空間法
  - kernel PCA を用いた方法。
などがある。

主成分分析における主成分得点データを説明変数として、他の機械学習推定アルゴリズムにかけて、
そのパターンを学習させるということも当然考えうる。
以下はこれまで同様、正規化した片目のデータ eyes200.sexp に主成分分析を施し、
各次元数に次元削減したデータを random-forest で学習し、顔推定検証を行った結果である。

次元数: 5, 正解率(min max mean): 0.90 0.95 0.92
次元数: 10, 正解率(min max mean): 0.87 0.96 0.91
次元数: 15, 正解率(min max mean): 0.85 0.97 0.93
次元数: 20, 正解率(min max mean): 0.93 0.99 0.96
次元数: 25, 正解率(min max mean): 0.88 0.98 0.92
次元数: 30, 正解率(min max mean): 0.90 0.97 0.93
次元数: 35, 正解率(min max mean): 0.90 0.98 0.94

次元数 5 の段階から平均正解率 90 %以上と高い値を示しており、次元数 20 でピークをむかえている。
検証用データに含まれていて、学習用データに含まれない人物の顔がある場合などがありえるため、
正解率 99, 98 %などはほぼ完璧に識別できているとみなしてよい。
ただ固有顔法よりも推定器の生成(学習)に時間がかかってしまうのが難点であるが、
いったん学習してしまえば固有顔法同様、推定は一瞬である。

*** 固有顔法
**** 学習
固有顔法における学習は細かく分けて二つの段階がある。

1. '顔'の学習
2. '顔-名'の学習

1. においてまず顔を解釈する枠組みを主成分分析を用いて構築し、
2. で、顔画像とその名前のついたデータセットを、その枠組みに投影し、顔画像を解釈した値と名前を対応させ、
推定器を作る。

より具体的に述べれば、
1. ではまずなるべくたくさんの顔画像（理想的には人類全て、名前はなくてよい）を主成分分析にかける。
その際データセットの列は顔画像における 各画素値 である。100 * 100 の解像度のデータを使えば 1 万列
のデータセットということになり、主成分も一万個求まる。主成分はあらかじめ決めた次元数分、または寄与率
（データの変動をどれだけ表現できているかの指標）によって絞る。
一般的には 100 から 150 ぐらいの主成分で十分個人を表現できると言われている。
これはなにをしているかと言うと、顔画像のどの部分の画素をどの程度重視し合成すればよいかということ、
'顔というのはどこをみればよいか'を学習している。

そうして選定した主成分を用いて 2. では、だれがどのような顔をしているかを学習する。
1. で学習した際と同じサイズの顔画像および名前のデータセットを、一個人に対して複数枚用意する。
このデータセットは 1. で学習したものとはまったく別物で構わない。
(画像を採取した環境はなるべく同じにすべきである)

各個人ごとに一枚ずつ、先に求めた主成分による主成分得点を算出し、
その平均値をその個人を代表する値（固有顔）とする。

**** 推定
推定の際には、推定対象データとその固有顔の距離を測り、一番小さかった固有顔の主の名前を推定値とする。

*** 部分空間法
パターン認識おける一般的方法で、高次元の特徴空間の中で，各クラスのデータが低次元の部分空間に
分布しているときに利用するクラス分類手法。主成分分析によって次元削減することで、顔認識に適用する。

部分空間法は固有顔法のように二段階に分けず、直接 '顔-名' のデータセットを学習する。

**** 学習
各個人ごとに複数枚の顔画像データを用意し、それらを主成分分析にかける。
すると各個人ごとに集められた顔画像データに特化した '顔を解釈する枠組み' (部分空間)が形成される。

**** 推定
推定する際は、推定対象データと学習の際に作られた部分空間との類似度を算出する。
一番類似している個人の名を推定値とする。
類似度の算出法はさまざまあり、現在MLに実装されているのは対象の顔ベクトルとそれを部分空間に射影し
たときのベクトルとの角度を評価するものである。

*** 相互部分空間法
**** 核非線形相互部分空間法

*** Note
- 100 * 100 の画素数のデータで主成分分析を行なうと STORAGE-CONDITION で落ちてしまう。
  よって現状は対象の右目に限った 60 * 28 のデータを用いているが、
  情報量が減ってしまうため、自然精度も落ちる。
  10000 もの主成分が必要なわけではなく、せいぜい多くて 100 であるので、条件に合わせて
  逐次的に主成分(固有対)を求める方法を導入すれば、 time, space が改善される見込み。
  主成分分析の次元数削減という目的からして全ての主成分が必要という状況は稀だと思われる。

- 顔認識は言い換えれば、多次元データのパターン認識、で応用範囲も広いと考えられる。
  主成分分析を用いない方法としては

  - Fisher-face
  - NMFを用いた顔認識
  - 隠れマルコフモデル

  などがある。
** 2010/1/13
顔認識の核非線形相互部分空間法を実現するために、カーネル主成分分析(kernel-princomp)の実装をした。
以下の二つの書籍/論文を参照した

1. パターン認識と機械学習下: ベイズ理論による統計的予測 著者: C.M.ビショップ
2. B.Schlkoph and A.J.Smola, Learning With Kernel:Section 5, MIT Press, 2002. 

2. は数式に不明な部分があるが、それに従うと kernel-princomp に関しては R と同様の結果が導きだせる。
しかし以下の二点の問題がある。

- kernel-princomp によって求めた空間上に点を射影(princomp-projection)する部分の結果が R と合わない。
- 線形カーネル(+linear+)による結果が主成分分析(princomp)と一致しない。

特に二点目に関しては、カーネル主成分分析の原理上必然な結果であるため、特に必要な内容かと思われ、
R のカーネル主成分分析にこそ問題があると考えられる。
1. に記述されている方法は明解であり、上記二点目の問題もクリアーしているため、R と結果が合わないことを
除けば問題はない。R とどの程度違うのか、主成分得点をプロットし散布図を描いて比較したところ、分布の概観
は一致しており、定性的には一致していると考えられるので、1. による実装を採用することにする。

** 2009/12/15
正定値対象行列に対し、べき乗法を用いて固有値の大きい順に固有対を求める eigen-by-power を実装した。
それを用いて、指定した次元数または累積寄与率の閾値までの主成分分析を行なう sub-princomp も導入した。
これにより必要な主成分までの計算で終わることができるので、計算時間は短縮される。
以前の版の全体に対する princomp で 5 分かかっていた部分が、
sub-princomp を用いれば 20 秒程度（閾値次第）で終わる。

部分空間法の実装は sub-princomp を用いるように変更した。
固有顔法においても sub-princomp の結果を用いることができる。

** 2009/12/10
顔推定の部分空間法の最も古典的な方法 CLAFIC について実装作業を進めている。期待通りの精度が出ていない。
部分空間法だと各個人のデータそれぞれでPCAをするため、固有顔に比して大幅に時間がかかる。

主成分分析について
  - 顔推定では学習時に引数として、主成分の次元に対する閾値があり、使用する固有ベクトルを限定している。
    逐次的に固有値の大きい順に固有対を求め、必要な分だけ求めたらやめる、というような方法があれば
    計算時間は縮まるのではないか（例えば固有顔では1680次元あるうちで必要なのは多くても100次元程度）

NMF の顔推定への応用
  - NMF によって PCA と同様にして顔推定（パターン推定）が実現できそうである。
    NMFはPCAに比して高速である見込み。精度はどうか。

** 2009/12/9
固有顔学習で推定器を作るコードにバグがあり、推定結果が正確ではなかった。
修正したものでの実行例が以下

PCA(4): (setq eyes-org
          (read-data-from-file "sample/eyes200.sexp" :external-format :shiftjis))
#<UNSPECIALIZED-DATASET>
DIMENSIONS: id | personID | p1 | p2 | p3 | p4 | p5 | p6 | p7 | p8 ...
TYPES:      UNKNOWN | UNKNOWN | UNKNOWN | UNKNOWN | UNKNOWN | UNKNOWN | UNKNOWN | UNKNOWN | UNKNOWN | UNKNOWN ...
DATA POINTS: 200 POINTS

PCA(6): (setq learn-eyes
          (pick-and-specialize-data eyes-org :except '(0 1)
                                    :data-types (make-list 1680 :initial-element :numeric))
          learn-person-eyes
          (pick-and-specialize-data eyes-org :except '(0)
                                    :data-types
                                    (cons :category (make-list 1680 :initial-element :numeric))))
#<NUMERIC-AND-CATEGORY-DATASET>
DIMENSIONS: personID | p1 | p2 | p3 | p4 | p5 | p6 | p7 | p8 | p9 ...
TYPES:      CATEGORY | NUMERIC | NUMERIC | NUMERIC | NUMERIC | NUMERIC | NUMERIC | NUMERIC | NUMERIC | NUMERIC ...
CATEGORY DATA POINTS: 200 POINTS
NUMERIC DATA POINTS: 200 POINTS

PCA(7): (princomp learn-eyes :method :covariance)
#<PCA-RESULT @ #x2cd0f312>
#<PCA-MODEL @ #x2cd132da>

PCA(8): (setq pca-result (first /) pca-model (second /))
#<PCA-MODEL @ #x2cd132da>

PCA(9): (make-face-estimator learn-person-eyes pca-result pca-model)
Dimension : 3
Number of self-misjudgement : 45
#<Closure (:INTERNAL
           (METHOD MAKE-FACE-ESTIMATOR
            (NUMERIC-AND-CATEGORY-DATASET T T))
           2)
  @ #x2d057a8a>
#<EQUALP hash-table with 9 entries @ #x2d04c952>

contribution-thld で累積寄与率への閾値を設定してやることで、
どこまでの主成分を使って固有顔を表現するかが指定できる。
自然高くすれば、精度は良くなり、誤判別数(Number of self-misjudgement)も減る。

PCA(10): (make-face-estimator learn-person-eyes pca-result pca-model
                              :contribution-thld 0.9d0)
Dimension : 9
Number of self-misjudgement : 32
#<Closure (:INTERNAL
           (METHOD MAKE-FACE-ESTIMATOR
            (NUMERIC-AND-CATEGORY-DATASET T T))
           2)
  @ #x2d5ca1f2>
#<EQUALP hash-table with 9 entries @ #x2d5be18a>
PCA(11): (make-face-estimator learn-person-eyes pca-result pca-model
                              :contribution-thld 0.95d0)
Dimension : 20
Number of self-misjudgement : 26
#<Closure (:INTERNAL
           (METHOD MAKE-FACE-ESTIMATOR
            (NUMERIC-AND-CATEGORY-DATASET T T))
           2)
  @ #x2db4534a>
#<EQUALP hash-table with 9 entries @ #x2db3770a>

eyes200.sexp は右目のみのデータに限っているので、データを増やせば精度が上がるという前提に立てば、
この精度(次元数 20 で認識率 87 %)は期待通りではなかろうか。一般には、画像が正規化（姿勢・位置・照明）
されれば、固有顔法で 100% 近い個人の同定が可能であると言われている。
(ref: 坂野 鋭” パターン認識における主成分分析”, 統計数理(2001))

しかし、現実にはそのような画像を採取するにはコストがかかる。よって
それらの問題に対処したさらに精度の高い認識手法として、
さらに主成分分析を駆使した手法である
部分空間法 や、
核非線形相互部分空間法 などがあり、
現状の ML の道具で実現できそうである。

問題点
 - 先述のごとく、環境 windowsXP 32bit, ACL 以下での princomp は分析対象データの列数 3000 ぐらいを
   境にして STORAGE-CONDITION で落ちてしまう。
   速度も列数 1680 のデータに対して 
PCA(7): (time (princomp learn-eyes :method :covariance))
; cpu time (non-gc) 307,438 msec (00:05:07.438) user, 62 msec system
; cpu time (gc)     484 msec user, 0 msec system
; cpu time (total)  307,922 msec (00:05:07.922) user, 62 msec system
; real time  338,485 msec (00:05:38.485)
; space allocation:
;  10,839 cons cells, 85,552,720 other bytes, 0 static bytes

   であるのに対し、
   VMS では 主成分分析: CPU 時間= 80.453000 秒, 経過時間=81 秒 である。
   98% を固有値計算に費やしているので、ライブラリが改善されれば大幅に
   改善される見込み。

** 2009/12/7
pca の解析例として、固有顔による顔認識の基本的な部分を実装

問題点
 - 学習時に用いるデータは列数が膨大となる。現状の主成分分析では (100 * 100) の画素のデータを食わせると
   STORAGE-CONDITION で落ちてしまう。よって、データを目などにフォーカスして小さくしてやらなければならない。
   しかし小さくすると情報量も落ちるので判別精度も落ちてしまう。

   -> 逆にそれぞれの顔の部位で推定器を生成してやり、その合議を以って結論とする方法などが考えられる。
 
 - 顔サンプルデータの明暗の度合いや距離が様々であることが、精度が伸びない一つの原因と考えられる。

** 2009/12/2
標準的季節調整モデルを導入
 - 時系列が トレンド成分 + 季節成分 で成り立つものと仮定したモデル
   - whard.sexp のようなデータに対しては非常に誤差の少ないモデル・予測を与えることができる。
   - msi-access などのようなトレンドが波打つようなデータにはフィッティングはうまくいくが、
     予測誤差幅が大きい。データ最終地点のトレンドに大きく左右されるためである。
     -> トレンド成分に季節成分を付与したように、さらに AR 成分を付与する。
        これにより、トレンド成分がさらに長期的・短期的成分に分解されることになる。

A 成分 + B 成分 + ... というスキームがこれにより割りと簡単に実現できる。
観察・解析によって適切な成分を見出せれば、予測などに強力なのではないか。
** 2009/9/4
トレンドモデル
 - t^2 に対するモデルの振る舞いを修正、原著と一致。AIC の値が依然合わないが、
   そもそもサンプルとして用いているデータが不完全で原著と一致していないので
   その誤差の範囲と考える。
 - t^2 の値を AIC を用いて最適化する機能を導入。
   :opt-t^2 t とすると最適な t^2 の値を :t^2, :delta, :search-width を用いて
   決定する。探索される値は、
   |i| <= search-width なる i に対して、
   t^2 + delta * i (但し 0 以上) 

AR モデル
 - AR モデルをガウス型状態空間モデルとして組み替えた。モデル値は従来と一致。
   これにより、モデルの標準誤差も算出されるようになった。

** 2009/9/2
トレンドモデルを導入
 - トレンドモデルは、モデルの次数 k および 状態 の分散 t^2 を指定して、トレン
   ドの推定を行なう。
 - 現状、定性的にはデータに対して、良くトレンドを把握しているように観察されるが、
   t^2 に対する振る舞い・AIC の値 などが正確に原著とは一致しない。
 - statvis とは連携済み

ガウス型状態空間モデル
 - 実装：平滑化(smoothing)
** 2009/8/24
ガウス型状態空間モデル
 - カルマンフィルター, 長期予測, 対数尤度計算までは実装済み
   ここまでの内容で、具体的なモデルを乗せて、時系列値の予測
   は可能なので、テストも兼ねて各モデルを取り入れる作業に移る。

 - 未実装：平滑化、欠測値補完
 
AR モデル
 - 以前実装した yule-walker, burg の AR モデルパラメータ推定による
   パラメータで、状態空間モデルによって AR モデルを構成し、予測値を算出。
   R および :ts-autoregression の ar-prediction と結果は完全に一致。

ARMA モデル
 - 状態空間モデルによって ARMA モデルを表現する部分は実装済み
   指定された次数による自動パラメータ推定は未実装
   R と結果が合わない。以下作業ログ

R
> (model<-arima(UKgas,order=c(2,0,1)))
(model<-arima(UKgas,order=c(2,0,1)))

Call:
arima(x = UKgas, order = c(2, 0, 1))

Coefficients:
         ar1      ar2     ma1  intercept
      0.4051  -0.2467  0.8318   340.0822
s.e.  0.1058   0.1034  0.0580    32.2398

sigma^2 estimated as 23837:  log likelihood = -698.63,  aic = 1407.26
> predict(model, n.ahead=10)
predict(model, n.ahead=10)
$pred
         Qtr1     Qtr2     Qtr3     Qtr4
1987 728.8467 388.3598 263.7364 297.2447
1988 341.5621 351.2492 344.2409 339.0122
1989 338.6228 339.7550                  

$se
         Qtr1     Qtr2     Qtr3     Qtr4
1987 154.3938 245.5721 248.6928 250.6422
1988 251.6347 251.6383 251.6872 251.6981
1989 251.6982 251.6992                  

ml
TS-AR(19): (setq model (arma ukgas '(0.4051 -0.2467) '(0.8318)))
#<ARMA-MODEL @ #x2102dbba>
TS-AR(20): (map 'list #'ts-p-pos (subseq (ts-points (predict model :n-ahead 10)) 108))
(#(-905.9580195039973) #(-275.96374161603745) #(395.8568072385349)
 #(512.5917232245624) #(394.14290828807924) #(317.3607895835569)
 #(315.4776759411853) #(333.6569752890662) #(341.48597359046585)
 #(340.17266765324064))












